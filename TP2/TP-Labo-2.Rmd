---
title: "TP2 - Reconocimiento de Patrones"
author: "Elio Campitelli"
output: 
  bookdown::tufte_html2:
    default
bibliography: bibliography.bib
biblio-style: apalike-es
---

```{r setup, include = FALSE}
library(ggplot2)
library(data.table)
library(magrittr)
library(patchwork)
library(palmerpenguins)
library(kableExtra)


theme_set(ggthemes::theme_tufte(base_size = 14) +
            theme(plot.background = element_rect(fill = "#FEFFF8", colour = NA))
          
)
knitr::opts_chunk$set(tidy = FALSE, message = FALSE, warning = FALSE, 
                      cache = TRUE, cache.extra = 1, echo = FALSE)

label_null <- as_labeller(function(x) "")

pen_cols <- c(adelaida = "darkorange", 
              barbijo = "purple", 
              juanito = "cyan4")
scale_color_penguin <- scale_fill_manual(values = pen_cols, aesthetics = c("color", "fill"))

update_geom_defaults(GeomPoint, list(size = 0.5))
set.seed(42)
```


```{r}
data(penguins)
penguins <- na.omit(as.data.table(penguins)[, -c("island", "sex")]) %>% 
  .[, species := forcats::fct_recode(species, adelaida = "Adelie", 
                                     barbijo = "Chinstrap",
                                     juanito = "Gentoo")] 

setnames(penguins, colnames(penguins), c("especie", "culmen_longitud", "culmen_alto", "aleta_largo", "masa"))
```

# Datos

En este TP voy a estar usando la base de datos `penguins` del paquete de R **palmerpenguins** [@R-palmerpenguins]. Los datos recolectados por la Dra. Kristen Gorman en la Estación Palmer, consisten en mediciones de la longitud del culmen^[El culmen es la parte superior del pico de las aves ![](culmen_depth.png)], alto del culmen y la masa corporal de `r nrow(penguins)` pingüinos de las especies *Pygoscelis adeliae* (Pingüino de Adelaida), *Pygoscelis papua* (Pingüino Juanito), y *Pygoscelis antarcticus* (Pingüino barbijo).

```{r, fig.align="center", out.width="80%"}
knitr::include_graphics("penguins.png")
```


En la Tabla \@ref(tab:pen-glance) se muestran las primeras 3 mediciones para cada especie.

```{r pen-glance}
knitr::kable(penguins[, head(.SD, 3), by = especie], booktabs = TRUE,
             col.names = c("Especie", "Longitud de culmen [mm]", "Alto de culmen [mm]", 
                           "Longitud de la aleta [mm]", "Masa corporal [g]"),
             caption = "Primeras 3 entradas de cada especie en los datos utilizados") %>% 
  column_spec(2:5, width = "2cm")
```


La variable categórica a predecir va a ser la especie, y las posibles variables predictoras son las dimensiones del culmen, la longitud de la aleta y la masa corporal. Es decir, en principio es un espacio de dimensión 4. Como la idea es trabajar en $\mathbb{R}^2$, conviene explorar qué grado de separación permite cada combinación de dos variables. Esto se muestra en la Figura \@ref(fig:pairs) donde se grafican scatterplots para todas las combinaciones de dos variables con la especie representada con color. Mirando las densidades de probabilidad (gráficos en la diagonal) se puede ver que la longitud del culmen separa bastante bien entre pingüino de adelaida y el resto mientras que las otras variables separan bien al pingüino juanito. Por lo tanto, las combinaciones que incluyen la longitud del culmen (gráficos en la primera columna) separan bastante bien entre las tres especies, mientras que el resto de las combinaciones tienen algún grado de mezcla entre pingüino de adelaida y pingüino juanito.


```{r pairs, fig.cap = "Scatteplot de todas las combinaciones de variables posibles en $\\mathbb{R}^2$. En la diagonal, estimaciones de densidad de cada variable separadas por especies.", fig.width = 10, fig.height = 6, fig.fullwidth = TRUE}
ggplot(penguins, aes(.panel_x, .panel_y)) +
  geom_point(aes(color = especie), size = 0.1) +
  ggforce::geom_autodensity(aes(fill = especie), alpha = 0.6, position = "dodge") +
  ggforce::facet_matrix(vars(culmen_longitud:masa),
                        layer.diag = 2) +
  scale_color_penguin +
  theme(legend.position = "bottom")
```

Para hacer las cosas más interesantes, voy a seleccionar el espacio formdado por el largo de la aleta y la masa. En este espacio $\mathbb{R}^2$, los piingüinos juanitos se separan muy bien de los otros, pero los adelaida y los barbijo están mezclados y son imposibles de separar. 


```{r}
data <- penguins[, .(z = factor(especie), 
                     x = aleta_largo, 
                     y = masa)] %>% 
  na.omit()

x <- "Largo de la aleta [mm]"
y <- "Masa [g]"
z <- "Especie"

ejes <- list(labs(x = x, y = y, fill = z, color = z))
```



```{r densidad, fig.cap = "Separación del espacio a partir de estimar la densidad con un kernel gausiano de distintos anchos de banda."}
density2d <- function(x, y, group, bandiwdths, ...) {
  xy <- na.omit(data.table(x, y, group))
  xy_scaled <- scale(xy[, c("x", "y")])
  center <- attr(xy_scaled, "scaled:center")
  sd <- attr(xy_scaled, "scaled:scale")
  
  lims <- c(apply(xy_scaled, 2, range))
  
  base_bandwidth <- mean(c(MASS::bandwidth.nrd(xy_scaled[, 1]),
                           MASS::bandwidth.nrd(xy_scaled[, 2])))
  
  bandwidths <- bandiwdths*base_bandwidth
  
  xy_scaled <- as.data.table(xy_scaled)
  xy_scaled$group <- xy$group
  
  density <- function(x, y, h) {  
    d <- MASS::kde2d(x, y, h = h, lims = lims, n = 200)
    dimnames(d$z) <-  list(x = d$x, 
                           y = d$y)
    res <- reshape2::melt(d$z)  
    res$bandwidth <- h
    res
  }
  
  out <- xy_scaled[, rbindlist(lapply(bandwidths, function(h) density(x, y, h))), by = group]
  
  out[, x := x*sd[1] + center[1]]
  out[, y := y*sd[2] + center[2]]
  out[]
}


density <- data[, density2d(x, y, z, c(1/4, 1, 4, 10))]

ggplot(data, aes(x, y)) +
  geom_raster(data = density[, .(z = group[which.max(value)], 
                                 conf = max(value)/sum(value)), 
                             by = .(x, y, bandwidth)],
              aes(x, y, fill = z, alpha = conf)) +
  geom_contour(data = density[, .(z = group[which.max(value)], 
                                  conf = max(value)/sum(value)), 
                              by = .(x, y, bandwidth)],
               aes(x, y, z = conf, color = z, alpha = ..level..)) +
  scale_alpha_continuous(range = c(0.1, 0.4), guide = "none") +
  # metR::geom_contour2(data = density, aes(x, y, z = value, color = group), global.breaks = FALSE,
  #                     size = 0.3, alpha = 0.4) +
  geom_point(aes(color = z), size = 0.1) +
  scale_color_penguin +
  ejes +
  # coord_fixed(2) +
  # guides(fill = "none", color = "none") +
  facet_wrap(bandwidth~., ncol = 2, labeller = labeller(bandwidth = function(x) paste0("Ancho de banda = ",
                                                                                       signif(as.numeric(x), 3))))  +
  theme(legend.position = "bottom")
```
La Figura \@ref(fig:densidad) muestra distintas particiones posibles del espacio a partir de estimar las densidades de probabilidad conjunta de cada clase y asignando la clase con mayor densidad de probabilidad en cada punto^[Dada la diferencia de escalas de x e y, en principio el valor del ancho de banda no podría ser el mismo para ambas dimensiones. Para armonizarlas, se hicieron los cálculos en base a las variables estandarizadas]. Cuando el ancho de banda es muy pequeño, se observa que el clasificación sufre de overfitting. Las regiones clasifican perfectamente los datos observados, pero los límites de decisión se curva para rodear puntos aislados. Para un ancho de banda muy grande, los límites de decisión tidneden a rectas y se ve que las estimaciones de densidad son círculos concéntricos que ignoran la covarianza entre los datos.

# Clasificador cuadrático

La función `clasificador_cuadratico` genera un modelo lineal de clasificación que es esencialmente un modelo lineal multivariado donde las $K$ variables dependientes representan a las $K$ categorías usando one-hot encoding. Es decir, el modelo tiene $M$ predictores y $K$ predicciones, una para cada clase. La clasificación se hace asignando la clase que tiene el valor máximo. Como medida de la confianza del resultado, se toma la razón entre ese valor máximo y la suma de todos los valores asignados a cada clase)

La partición del espacio $\mathbb{R}^2$ elegido usando el resultado del clasificador cuadrático se muestra en la Figura \@ref(fig:cuadratico). Dado que el clasificador es lineal, las divisiones entre categorías son rectas que se interceptan en un punto central. 

```{r}
clasificador_cuadratico <- function(formula, data = environment(formula)) { 
  # one-hot encoding de las categorías
  data <- model.frame(formula, data)
  formula_hot <- paste0("~ 0 + ", deparse(formula[[2]]))
  hot <- model.matrix(as.formula(formula_hot), data = data)
  
  # datos de predictores
  bare_data <- model.matrix(formula, data)[, -1]
  bare_data_scaled <- scale(bare_data)
  
  center <- attr(bare_data_scaled, "scaled:center")
  sd <- attr(bare_data_scaled, "scaled:scale")
  
  bare_data_scaled <- cbind(1, bare_data_scaled)
  # fiteando el modelo
  model <- .lm.fit(bare_data_scaled, as.matrix(hot))
  
  structure(.Data = model$coefficients,
            levels = levels(data[[deparse(formula[[2]])]]),
            formula = formula, 
            center = center, 
            sd = sd,
            class = "cuadratico")
}


predict.cuadratico <- function(object, newdata, prediction = c("level", "confidence"), ...) {
  formula <- attr(object, "formula", TRUE)
  formula <- delete.response(terms(formula))
  sd <- attr(object, "sd", TRUE)
  center <- attr(object, "center", TRUE)
  
  bare_data <- as.matrix(model.frame(formula, newdata, na.action = NULL))
  bare_data <- cbind(1, scale(bare_data, center = center, scale = sd))
  
  levels <- attr(object, "levels", TRUE)
  pred <- bare_data %*% object
  if (prediction[1] == "level") {
    return(factor(levels[max.col(pred)], levels = levels)  )
  } else { 
    pred <- as.data.frame(pred)
    colnames(pred) <- levels
    return(as.list(pred))
  }
}
```


```{r cuadratico, fig.cap = "Clasificación en base a clasificador cuadrático lineal. En contornos negros, el nivel de confianza del modelo. \"Precisión\" se define como la proporción de observaciones clasificadas como una determinada especie que fueron clasificadas correctamente, \"Exhaustividad\" se define como la proporción de observaciones de cada especie correctamente clasificadas."}
W <- clasificador_cuadratico(z ~ x + y, data)

data$pred <- predict(W, data)

div <- expand.grid(x = seq(min(data$x), max(data$x), length.out = 140), 
                   y = seq(min(data$y), max(data$y), length.out = 140))

div$pred <- predict(W, div)

div$conf <- as.matrix(as.data.frame(predict(W, div, prediction = "confidence"))) %>% 
  apply(1, function(x) max(x)/sum(x))

data[] %>% 
  copy() %>% 
  .[, correct := pred == z] %>% 
  .[, mean(correct), by = z] %>% 
  .[, scales::percent(V1)] ->  exhaustividad

data[] %>% 
  copy() %>% 
  .[, correct := pred == z] %>% 
  .[, mean(correct), by = pred] %>% 
  .[, scales::percent(V1)] -> precision


medidas <- paste0("Precisión = ", precision, "\nExhaustividad = ", exhaustividad)


ggplot(data, aes(x, y)) +
  geom_raster(aes(fill = pred, alpha = conf), data = div) +
  geom_contour(aes(z = conf, alpha = ..level.., color = pred), data = div, size = 0.2) +
  scale_alpha_continuous(range = c(0.1, 0.4), guide = "none") +
  geom_point(aes(color = z)) +
  annotate(shadowtext:::GeomShadowText,
           label = medidas,
           x = c(175, 230, 215),
           y = c(5500, 4000, 3000),
           color = pen_cols[c(1, 3, 2)],
           hjust = c(0, 1, 1),
           vjust = c(1, 0, 0),
           bg.r = 0.20, bg.color = "#FEFFF8", size = 4.5,
           family = hrbrthemes::font_rc_light) +
  scale_color_penguin +
  ejes +
  # coord_fixed(2) +
  theme(legend.position = "bottom")
```

Se muestran dos medidas de la clasificación para cada especie. "Exhaustividad" es la proporción de observaciones que son clasificadas como una especie de forma correcta. "Precisión" es la proporción de observaciones de una determinada especie que son clasificadas correctamente. Es decir, la exhaustividad  del 100% para los pingüinos juanitos implica que la probabilidad de que un pingüino juanito sea correctamente clasificado es del 100%. Sin embargo, la precisión del 80% implica que si el modelo clasifica un pingüino como juanito, hay un 80% de probabilidad de que haya sido clasificado correctamente. 
Se puede ver que si bien el clasificador lineal cuadrático identifica sin problemas a los pingüinos juanito, la separación entre adelaida y barbijo no es para nada buena. 


## Logística

La función `logistica` realiza el ajuste logístico mediante un método iterativo. Soporta clasificación de múltiples clases usando el algoritmo de uno-vs-todos. Es decir, para $K$ clases genera $K$ modelos que dan la probabilidad de que una observación determinada pertenezca a la clase k-ésima o a cualquiera de las otras. La clasificación luego se hace tomando la clase que tiene la mayor probabilidad. Al igual que con el clasificador cuadrático, como medida de confianza del modelo se calcula la razón entre la probabilidad asignada a la clase ganadora y la suma de todas las probabilidades asignadas. 

```{r define-logistic}
sigmoid = function(x) {
  1 / (1 + exp(-x))
}


solve_sigmoid <- function(X, t, conv_tol = 1e-4, max_iter = 1e4) {
  W <- matrix(rep(0, ncol(X)), nrow = ncol(X), ncol = ncol(t))
  hist <- list(W)
  converged <- rep(FALSE, ncol(t))
  
  for (i in seq_len(max_iter)) {
    y <- sigmoid(t(t(W) %*% t(X)))
    y[y == 1] <- 1 - 1e-9
    y[y == 0] <- 0 + 1e-9
    
    for (j in seq_len(ncol(t))) {
      if (!converged[j]) {
        r <- as.vector(y[, j]*(1 - y[, j]))
        R <- diag(r, nrow = length(r))
        R_inv <- diag(1/r, nrow = nrow(y))
        Z <-  X %*% W[, j] - R_inv %*% (y[, j] - t[, j])
        W_j_new <- solve(t(X) %*% R %*% X) %*% t(X) %*% R %*% Z    
        rel_change <- abs(W[, j] - W_j_new)/abs(W[, j])
        converged[j] <- all(rel_change <= conv_tol)
        W[, j] <- W_j_new
      }
      hist[[i + 1]] <- W
    }
    
    if (all(converged)) {
      break
    }
    
  }
  
  list(W = hist, converged = converged)
}


logistica <- function(formula, data, conv_tol = 1e-4, max_iter = 1e4) {
  # one-hot encoding de las categorías
  data <- model.frame(formula, data)
  formula_hot <- paste0("~ 0 + ", deparse(formula[[2]]))
  t <- model.matrix(as.formula(formula_hot), data = data)
  
  # datos de predictores
  bare_data <- model.matrix(formula, data)
  
  sd <- apply(bare_data, 2, sd, na.rm = TRUE)
  sd[1] <- 1
  means <- apply(bare_data, 2, mean, na.rm = TRUE)
  means[1] <- 0
  
  bare_data_scaled <- scale(bare_data, center = means, scale = sd)
  
  center <- attr(bare_data_scaled, "scaled:center")
  sd <- attr(bare_data_scaled, "scaled:scale")
  
  W <- solve_sigmoid(bare_data_scaled, t, conv_tol = conv_tol, max_iter = max_iter)
  # browser()
  if (any(is.na(W$converged) || !W$converged)) {
    warning("Iteration didn't converge!")
  }
  
  structure(.Data = W$W,
            levels = levels(data[, 1]),
            formula = formula, 
            center = center, 
            sd = sd,
            converged = W$converged,
            class = "logistica")
}

predict.logistica <- function(object, 
                              newdata, type = c("prediction", "sigmoid"), n = length(object),
                              cutoff = 0.5) {
  formula <- attr(object, "formula", TRUE)
  formula <- delete.response(terms(formula))
  sd <- attr(object, "sd", TRUE)
  center <- attr(object, "center", TRUE)
  
  bare_data <- as.matrix(model.matrix(formula, newdata, na.action = NULL))
  bare_data <- scale(bare_data, center = center, scale = sd)
  
  levels <- attr(object, "levels", TRUE)
  
  lapply(n, function(i) {
    W <- object[[i]]
    # browser()
    pred <- sigmoid(t(t(W) %*% t(bare_data)))
    if (type[1] == "prediction") {
      pred <- factor(levels[max.col(pred)], levels = levels)
    } else {
      pred <- as.data.frame(pred)
      colnames(pred) <- levels
    }
    return(pred)
  })
}
```


```{r logistica-lineal, fig.cap = "Convergencia del modelo logístico lineal. En contornos negros, el nivel de confianza del modelo."}
model <- logistica(z ~ x + y, data = data)

div <- expand.grid(x = seq(min(data$x), max(data$x), length.out = 90), 
                   y = seq(min(data$y), max(data$y), length.out = 90))

preds <- predict(model, div, type = "sigmoid")

hist <- lapply(preds, function(p) {
  d <- div
  class <- max.col(p)
  d$pred <- factor(colnames(p)[class], levels = attr(model, "levels"))
  d$conf <- apply(as.matrix(p), 1, function(x) max(x)/sum(x))
  d
}) %>% 
  rbindlist(idcol = "iter")


ggplot(hist[iter == max(iter)], aes(x, y)) +
  geom_raster(aes(fill = pred, group = 1, alpha = conf), interpolate = TRUE) +
  geom_contour(aes(z = conf, alpha = ..level.., color = pred)) +
  scale_alpha_continuous(range = c(0.1, 0.4), guide = "none") +
  geom_point(aes(color = z), data = data) +
  scale_color_penguin +
  ejes +
  labs(fill = "Especie", color = "Especie") +
  theme(legend.position = "bottom")
```

La Figura \@ref(fig:logistica-lineal) muestra el resultado de la clasificación logística lineal. Al igual que con la Figura \@ref(fig:cuadratico), los límites de decisión son rectas dado qu eel modelo es linear en este espacio $\mathbb{R}^2$. La partición no es muy distinta de la partición usando el clasificador cuadrático. La Figura \@ref(fig:logistica-eliptica), en cambio, muestra el resultado del modelo logístico pero aplicado al espacio de dimensión 4 $(x, y, x^2, y^2, xy)$. Al tener términos no lineales, los límites de decisión ahora pueden ser curvos^[Son curvos en el espacio $\mathbb{R}^2$ mostrado, en el espacio de dimensión 4, siguen siendo rectas.]

```{r logistica-eliptica, fig.cap = "Igual que la Figura \\@ref(fig:logistica-lineal) pero para un ajuste elíptico."}
model <- logistica(z ~ poly(x, y, degree = 2, raw = TRUE), data = data)

div <- expand.grid(x = seq(min(data$x), max(data$x), length.out = 90), 
                   y = seq(min(data$y), max(data$y), length.out = 90))

preds <- predict(model, div, type = "sigmoid")


hist <- lapply(preds, function(p) {
  d <- div
  class <- max.col(p)
  d$pred <- factor(colnames(p)[class], levels = attr(model, "levels"))
  d$conf <- apply(as.matrix(p), 1, function(x) max(x)/sum(x))
  d
}) %>% 
  rbindlist(idcol = "iter")


ggplot(hist[iter == max(iter)], aes(x, y)) +
  geom_raster(aes(fill = pred, group = 1, alpha = conf), interpolate = TRUE) +
  geom_contour(aes(z = conf, alpha = ..level.., color = pred)) +
  scale_alpha_continuous(range = c(0.1, 0.4), guide = "none") +
  geom_point(aes(color = z), data = data) +
  scale_color_penguin +
  ejes +
  labs(fill = "Especie", color = "Especie") +
  theme(legend.position = "bottom")
```




# Expectation Maximisation

Expectation maximisation (EM) es una técnica no supervisada, es decir, que no tiene en cuenta las clases observadads. 

```{r}
EM <- function(data, k, n_iter = 1e5, tol = 1e-6) {
  # k <- 3 # cantidad de clases
  ks <- seq_len(k)
  M <- ncol(data) # cantidad de features
  N <- nrow(data) # cantidad de obs
  D <- scale(as.matrix(data))
  
  center <- attr(D, "scaled:center")
  sd <- attr(D, "scaled:scale")
  
  pi <- rep(1/k, k)
  mu <- lapply(ks, function(x) rnorm(M))
  sigma <- lapply(ks, function(x) diag(1/k^2, nrow = M))
  log_ver <- Inf
  h <- log_ver
  # resp_hist <- list()
  mu_hist <- list()
  sigma_hist <- list()
  distr_hist <- list()
  
  
  for (iter in seq_len(n_iter)) {
    resp <- lapply(ks, function(i) {
      pi[[i]]*mvtnorm::dmvnorm(D, mean = mu[[i]], sigma = sigma[[i]])
    })
    
    resp_norm <- Reduce("+", resp)
    resp <- lapply(ks, function(i) resp[[i]]/resp_norm)
    
    # resp_hist[[iter]] <- lapply(resp, function(r) {
    #   d <- data
    #   d$resp <- r
    #   d}) %>% 
    #   rbindlist(idcol = "class") 
    
    
    Nk <- unlist(lapply(ks, function(i) sum(resp[[i]])))
    pi <- Nk/N
    mu <- lapply(ks, function(i) colSums(D * resp[[i]])/Nk[[i]])
    sigma <- lapply(ks, function(i) t(D - mu[[i]]) %*% ( (D - mu[[i]]) * resp[[i]] )/Nk[[i]])
    
    distr_hist[[iter]] <- lapply(ks, function(i) {
      list(sigma = sigma[[i]]*(sd %*% t(sd)),
           mu = mu[[i]]*sd + center )})
    
    mu_hist[[iter]] <- as.data.table(transpose(mu))[, class := 1:.N]
    sigma_hist[[iter]] <- sigma
    
    log_ver_new <- lapply(ks, function(i) {
      pi[[i]]*mvtnorm::dmvnorm(D, mean = mu[[i]], sigma = sigma[[i]])
    }) %>% 
      Reduce("+", .) %>% 
      log() %>% 
      sum()
    
    
    h <- c(h, log_ver_new)
    
    converged <- abs((log_ver - log_ver_new)/(log_ver_new)) <= tol
    
    if (converged) {
      break
    }
    log_ver <- log_ver_new
  }
  structure(.Data = distr_hist, 
            center = center,
            sd = sd)
}
# }

```



```{r em-pinguinos, fig.cap = "Media y elipse normal "}
set.seed(123)
distr_hist <- EM(data[, .(x, y)], k = 3)

elipses <- lapply(distr_hist, function(i) {
  lapply(i, function(k) {
    as.data.table(ellipse::ellipse(k$sigma, centre = k$mu))
  }) %>% 
    rbindlist(idcol = "class")
}) %>% 
  rbindlist(idcol = "iter")

centers <- lapply(distr_hist, function(i) {
  lapply(i, function(k) {
    # browser()
    k$mu
  }) %>% 
    do.call(rbind, .) %>% 
    as.data.table() %>% 
    .[, class := 1:.N] %>% 
    .[]
}) %>% 
  rbindlist(idcol = "iter")

ultimo <- distr_hist[[length(distr_hist)]]

div <- expand.grid(x = seq(min(data$x), max(data$x), length.out = 140), 
                   y = seq(min(data$y), max(data$y), length.out = 140))
density <- lapply(ultimo, function(k) {
  mvtnorm::dmvnorm(as.matrix(div), mean = k$mu, sigma = k$sigma)
})

div$conf <- apply(as.matrix(as.data.table(density)), 1, function(x) max(x)/sum(x))
div$pred <- factor(max.col(as.matrix(as.data.table(density))))



div %>% 
  ggplot(aes(x, y)) +
  geom_raster(aes(fill = pred, group = 1, alpha = conf), interpolate = TRUE) +
  geom_contour(aes(z = conf, alpha = ..level.., color = pred)) +
  scale_alpha_continuous(range = c(0.1, 0.4), guide = "none") +
  
  geom_path(data = elipses[iter == max(iter)],
            aes(group = class, color = factor(class))) +
  geom_point(data = centers[iter == max(iter)], aes(color = factor(class)), size = 3) +
  
  scale_fill_brewer("k", palette = "Set2") +
  scale_color_brewer("k", palette = "Set2") +
  
  ggnewscale::new_scale("color") +
  ggnewscale::new_scale("fill") +
  
  geom_point(data = data, aes(color = factor(z))) +
  
  ejes +
  labs(x = NULL) +
  scale_color_penguin +
  theme(legend.position = "bottom")
```
La Figura \@ref(fig:em-pinguinos) muestra el resultado del algoritmo de EM con los datos de pingüinos con 2 clases. Los puntos son las medias de las distribuciones normales y las elipses marcan la el cuartil del 95%. Escala de colores de las 3 clases y las 3 especies no son las mismmas ya que no hay concordancia entre ambas necesariamente. El espacio se particiona entre las 3 clases a partir de la densidad de probabilidad normal de cada clase encontrada por EM y tomando la clase que maximiza la misma. 

La clase 3 (celeste) coincide bien con los pingüinos juanito, ya que éstos se separan bien del resto, pero las clases 1 y 2 no representan bien las especies adelaida y barbijo. 


## Fisher

```{r}
data_bk <- data
```



```{r}
data <- data_bk
formula <- z ~ x + y

fischer <- function(group, ...) {
  data <- cbind(data.table::data.table(group = group), 
                data.table::data.table(...))
  data <- na.omit(data)
  
  
  data_split <- split(data, data$group, drop = TRUE)
  
  
  means <- lapply(data_split, function(dat) {
    X <- dat[, -1]
    colMeans(X)
  })
  
  
  S <- lapply(seq_along(data_split), function(i) {
    cov(as.matrix(data_split[[i]][, -1])) * (nrow(data_split[[i]]) - 1)
  })
  
  W <- solve(Reduce("+", S)) %*% Reduce("-", means)
  structure(.Data = W, 
            class = "fischer",
            levels = levels(data),
            pred_names = colnames(data))
}

predict.fischer <- function(object, newdata) {
  
  data <- newdata[, attr(object, "pred_names", TRUE)]
  proj <- as.matrix(data) %*% object
  
}

```


```{r}
d <- data[z != "juanito"]

f <- with(d, fischer(z, x, y))

d$proj <- as.matrix(d[, .(x, y)]) %*% f

```


```{r}
ggplot(d, aes(x, y)) +
  geom_point(aes(color = z))

ggplot(d, aes(proj)) +
  geom_density(aes(color = z, fill = z), alpha = 0.5) +
  geom_rug(aes(color = z)) +
  scale_color_penguin 
```


# Referencias