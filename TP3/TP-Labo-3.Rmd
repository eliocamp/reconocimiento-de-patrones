---
title: "TP3 - Reconocimiento de Patrones"
author: "Elio Campitelli"
output: 
  bookdown::tufte_handout2:
    toc: no
    number_sections: no
biblio-style: apalike-es
---

```{r setup, include = FALSE, tidy = FALSE}
library(ggplot2)
library(data.table)
library(magrittr)
library(patchwork)
library(palmerpenguins)
library(kableExtra)


theme_set(ggthemes::theme_tufte(base_size = 14) +
            theme(plot.background = element_rect(fill = "#FEFFF8", colour = NA))
          
)
knitr::opts_chunk$set(tidy = FALSE, message = FALSE, warning = FALSE, tidy = FALSE,
                      dev = "cairo_pdf",
                      cache = TRUE, cache.extra = 42, echo = FALSE)

label_null <- as_labeller(function(x) "")


update_geom_defaults(GeomPoint, list(size = 0.5))
source(here::here("TP3", "functions.R"))
set.seed(42)
```


## SVM con separación lineal

Voy a implementar SVM usando soft-margin para datos sintéticos generados a partir de dos distribuciones normales bivariadas con $\Sigma^2=I$ y distintos grados de separación (\$\\delta \\in {1, 2, 4, 6}\$) en dirección x. La recta de separación óptima es una recta vertical centrada en cero.

```{r}
deltas <- c(6, 4, 2, 1)
names(deltas) <- deltas
sims <- lapply(deltas, function(x) {
  generate_data(ns = c(150, 100), 
                mus = list(c(-x/2, 0), 
                           c(x/2, 0)),
                sigma= list(diag(1, 2), 
                            diag(1, 2)))
}) %>% 
  rbindlist(idcol = "delta") %>% 
  .[, delta := as.numeric(delta)]


lambdas <-  c(1, 0.1, 0.0001)
names(lambdas) <- lambdas

models <- lapply(lambdas, function(l) {
  sims[, .(model = list(SVM(id ~ x + y, lambda = l, epochs = 100)),
           data = list(.SD)), 
       by = delta]
}) %>% 
  rbindlist(idcol = "lambda") %>% 
  .[, lambda := as.numeric(lambda)]

models[, lambda_e := factor(paste0("lambda == ", lambda))]
models[, delta_e := factor(paste0("delta == ", delta))]

fields <- models[, predict_field(model[[1]], x = c(-5, 5), y = c(-3, 3), n = 30), 
                 by = .(lambda_e, delta_e)]
```


```{r deltas, fig.cap = "Límites de decisión para datos sintéticos con distintos grados de separación (columnas) y distinto nivel de tolerancia para datos mal clasificados (filas).", fig.height=3}


ggplot(mapping = aes(x, y)) +
  geom_contour_filled(aes(z = lev,  fill = sign(..level_mid..)),
                      data = fields, alpha = 0.4)  +
  geom_point(aes(color = id), size = 0.2,
             data = models[, data[[1]], by = .(lambda_e, delta_e)]) +
  metR::geom_contour2(data = fields, aes(z = lev), color = "black", breaks = 0) + 
  scale_color_brewer(palette = "Dark2", guide = "none") +
  scale_fill_gradient2(low = "#1B9E77", high = "#D95F02", guide = "none") +
  scale_x_continuous(NULL) +
  scale_y_continuous(NULL) +
  facet_grid(lambda_e ~ delta_e, 
             labeller = labeller(lambda_e = label_parsed, delta_e = label_parsed)) 
```

En la Figura \ref{fig:deltas} se muestra el límite de decisión para los cuatro set se datos (columnas) y los grados de soft-margin donde un $\lambda$ pequeño equivale a poca tolerancia mientras que un lambda grande equivale a mucha tolerancia.

Se ve que para datos muy separados el clasificador es poco sensible a la elección de hiperparámetro $\lambda$. Para datos poco separables  la elección de \$\\lambda\$ tiene gran impacto en el resultado.

## SVM con kernel gaussiano

Luego, extiendo el algoritmo PEGASOS para poder usar un kernel arbitrario. En la Figura \ref{fig:svm-gauss} se muestran los resultados de applicar SVM con kernel gaussiano con distintos valores de $\gamma$ y $\lambda$ y un $\delta = 3.5$. 

```{r}
set.seed(42)
deltas <- c(3.5)
names(deltas) <- deltas
sims <- generate_data(ns = c(150, 150), 
                      mus = list(c(-deltas/2, 0), 
                                 c(deltas/2, 0)),
                      sigma= list(diag(1, 2), 
                                  diag(1, 2)))

lambdas <-  10^seq(-3, 2, by = 2)
gammas <- 10^(seq(-3, 2, by = 1))
params <- CJ(lambda = lambdas, 
             gamma = gammas)


models_k <- params[, .(model = list(SVM(id ~ x + y, data = sims,  epochs = 300, 
                                        lambda = lambda,kernel = gauss_kernel(gamma)))), 
                   by = .(lambda, gamma)]

models_k[, lambda_e := factor(paste0("lambda == ", lambda))]
models_k[, gamma_e := factor(paste0("gamma == ", gamma))]

fields <- models_k[, predict_field(model[[1]], x = c(-5, 5), y = c(-3, 3), n = 40),
                   by = .(lambda_e, gamma_e)]
fields[, lev_r := lev/max(lev), by = .(lambda_e, gamma_e)] 
```

```{r svm-gauss, fig.cap = "SVM con kernel gaussiano para distintos valores de $\\gamma$ y $\\lambda$. La separación entre los datos es $\\delta = 2$.", fig.height=6, fig.fullwidth = TRUE, fig.width=9}
ggplot(mapping = aes(x, y)) +
  
  geom_contour_filled(aes(z = lev_r,  fill = sign(..level_mid..)),
                      data = fields, alpha = 0.4)  +
  geom_point(aes(color = id), data = sims) +
  geom_contour(aes(z = lev), breaks = 0, color = "black",
               data = fields)  +
  scale_color_brewer(palette = "Dark2", guide = "none") +
  scale_fill_gradient2(low = "#1B9E77", high = "#D95F02", guide = "none") +
  facet_grid(lambda_e ~ gamma_e, labeller = label_parsed)
```


Estos datos sintéticos son un poco aburridos. Voy a usar los datos que se muestran en la Figura 7.2 del Bishop (Figura \ref{fig:bishop2} derecha). Digitalicé los datos y los usé para entrenar una SVM con kernel gausiano con $\gamma = 0.5$ y $\lambda = 0.1$. El resultado se muestra en la Figura \ref{fig:bishop2} izquierda, que es muy similar. 


```{r}
set.seed(42)
bishop <- fread(here::here("TP3", "bishiop7.2.csv")) %>% 
  melt(id.vars = c("x"), value.name = "y", variable = "id") %>% 
  .[, id := forcats::fct_recode(id, "1" = "Curve1", "2" = "Curve2")] %>% 
  na.omit() 
bishop_svm <- SVM(id ~ x + y, data = bishop, lambda = 0.001, epochs = 300,
                  kernel = gauss_kernel(0.5))
# bishop_svm$a[bishop_svm$a == min(bishop_svm$a)] <- 0
field <- predict_field(bishop_svm, bishop$x, bishop$y, 40)
```

```{r bishop2, fig.cap = "SVM con kernel gaussiano para los datos de la Figura 7.2 del Bishop. y(x) en líneas de contorno, y(x) = 0 en línea gruesa.A la derecha, la figura original, a la izquierda, el clasificador entrenado en este ejercicio", fig.height=4, fig.width=9, fig.fullwidth = TRUE, dependson=field}

g <- ggplot(bishop, aes(x, y)) +
  geom_contour(data = field, aes(z = lev), color = "black", size = 0.1) +
  geom_contour(data = field, aes(z = lev), color = "black", breaks = 0) +
  # geom_contour(data = field, aes(z = lev), color = "black", breaks = c(-1, 1)) +
  geom_point(aes(color = id), shape = 4, size = 2) +
  geom_point(data = bishop[bishop_svm$SV], shape = 21, size = 4, color = "green") +
  scale_color_manual(guide = "none", values = c("1" = "blue", "2" = "red")) +
  scale_x_continuous(NULL, breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0))

p2 <- cowplot::ggdraw() +
  cowplot::draw_image(here::here("TP3", "bishop_screncap.png"))

cowplot::plot_grid(p2, g, labels = c("Bishop (7.2)", "Fit"))
```


## Crossvalidation


```{r cv}
lambdas <-  10^seq(-3, 2, by = 0.5)
gammas <- 10^(seq(-2, 2, by = 0.25))
params <- CJ(lambda = lambdas, 
             gamma = gammas)

N <- 50
delta <- 3
set.seed(42)
datos <- generate_data(ns = c(N, N), 
                     mus = list(c(-delta/2, 0), 
                                c(delta/2, 0)),
                     sigma= list(diag(1, 2), 
                                 diag(1, 2)))

rmse_cv <- function(datos, k_fold = 10, lambda, gamma, epochs = 100) {
  N <- nrow(datos)
  grupos <- ggplot2::cut_number(seq_len(N), k_fold)
  
  rmses <- vapply(seq_len(k_fold), function(k) {
    train_index <- grupos != levels(grupos)[k] 
    train <- datos[train_index == TRUE, ]
    validation <- datos[train_index == FALSE, ]
    
    model <- SVM(id ~ x + y, data = train, epochs = epochs, 
                 lambda = lambda,
                 kernel = gauss_kernel(gamma))
    validation$pred <- predict(model, validation)$pred
    mean(validation$pred == validation$id)
    
  }, numeric(1))
  return(mean(rmses))
}

reps <- 20
positive <- lapply(seq_len(reps), function(i) params[, rmse_cv(datos, 10, lambda, gamma), by = .(lambda, gamma)]) %>% 
  rbindlist() %>% 
  .[, mean(V1), by = .(lambda, gamma)]
best <- positive[V1 == max(V1)]
```

```{r cv-fig, fig.cap = "Acierto computado a partir de validación cruzada para distintos valores de $\\lambda$ y $\\gamma$", fig.height=4}
positive %>% 
  copy() %>% 
  .[, c("dl", "dg") := metR::Derivate(V1 ~ log(lambda) + log(gamma), fill = TRUE)] %>% 
  ggplot(aes(gamma, lambda)) +
  metR::geom_contour_fill(aes(z = V1), bins = 20) +
  metR::geom_streamline(aes(dx = dg, dy = dl), res = 3, n = 10,
                        L = 30, alpha = 0.5,
                        size = 0.05) +
  geom_point(alpha = 0.5, size = 0.1) +
  geom_point(data = best, size = 1) +
  ggrepel::geom_text_repel(data = best, parse = TRUE, bg.colour = "#DFE234", bg.r = 0.2,
                           nudge_x = 0.3, nudge_y = 0.3,
                           aes(label = paste0("gamma == ", signif(gamma, 2), 
                                              "~~ lambda == ", lambda))) +
  scale_y_continuous(latex2exp::TeX("$\\lambda"), trans = scales::log10_trans(),
                     # breaks = unique(positive$lambda),
                     expand = c(0.01, 0)) +
  scale_x_continuous(latex2exp::TeX("$\\gamma$"),
                     # breaks = unique(positive$gamma), 
                     trans = scales::log10_trans(), 
                     expand = c(0.01, 0)) +
  scale_fill_viridis_c("Tasa de acierto") +
  theme(legend.position = "bottom")

```
La Figura \ref{fig:cv-fig} muestra la tasa de acierto (proporción de observaciones correctamente categorizadas) computada por validación cruzada para distintos valores de $\lambda$ y $\gamma$. La mejor combinación de hiperparámetros está marcada con un punto. En la Figura \ref{fig:field-best} se muestra el límite de decisión para la SVM ajustada con éstos. 

```{r field-best, fig.cap = "SVM fiteada con los mejores valores de $\\lambda$ y $\\gamma$ elegidos por validación cruzada."}
model <- SVM(id ~ x + y, data = datos, lambda = best$lambda, kernel = gauss_kernel(best$gamma), epochs = 100)

field <- predict_field(model, c(-5, 5), c(-3, 3))

ggplot(mapping = aes(x, y)) +
  geom_contour_filled(aes(z = lev,  fill = sign(..level_mid..)),
                      data = field, alpha = 0.4)  +
  geom_point(aes(color = id), data = datos) +
  geom_point(data = datos[model$SV], shape = 21, size = 3, color = "green") +
  geom_contour(aes(z = lev), breaks = 0, color = "black",
               data = field)  +
  scale_color_brewer(palette = "Dark2", guide = "none") +
  scale_fill_gradient2(low = "#1B9E77", high = "#D95F02", guide = "none") 
```

